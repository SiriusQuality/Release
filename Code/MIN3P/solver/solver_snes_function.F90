!*****Revision Informations Automatically Generated by VisualSVN*****!
!---------------------------------------------------------------------
!> $ID:$
!> $Revision: 453 $
!> $Author: dsu $
!> $Date: 2017-02-21 19:54:05 +0100 (Tue, 21 Feb 2017) $
!> $URL: https://biot.eos.ubc.ca/svn/min3p_thcm/branches/fgerard_new/src/solver/solver_snes_function.F90 $
!---------------------------------------------------------------------
!********************************************************************!

#ifdef PETSC

!> module: solver_snes_function
!>
!> written by: Danyang Su
!>
!> module description:
!>
!> Module of nonlinear function  
!>
!> Note: This module is not high efficient at present. 
!> See http://www.mcs.anl.gov/petsc/ for detail


module solver_snes_function

      implicit none
    
      contains
    
      !> Evaluate initial guess
      subroutine form_initial_guess(rank,da,x_array_loc,x_vec_loc,     &
                     x_vec_gbl,nngl,row_idx_l2pg,col_idx_l2pg,         &
                     b_non_interlaced)
         
          implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscdmda.h90>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscis.h>
#include <finclude/petscvec.h>
#include <finclude/petscvec.h90>
#include <finclude/petscmat.h>
#include <finclude/petscpc.h>
#include <finclude/petscksp.h>
#include <finclude/petscdmda.h>
#include <finclude/petscdmda.h90>
#endif

          PetscInt :: rank
          DM :: da 
          PetscReal, allocatable :: x_array_loc(:)
          Vec :: x_vec_loc
          Vec :: x_vec_gbl
          PetscInt :: nngl
          PetscInt, allocatable :: row_idx_l2pg(:)
          PetscInt, allocatable :: col_idx_l2pg(:)
          PetscBool :: b_non_interlaced
          
          PetscInt :: info_debug
          PetscInt :: i, j
          PetscErrorCode :: ierr
          PetscScalar, pointer :: vecpointer(:)          
          
          
          !By default, PETSc use interlaced vector.
          !The reactive transport use interlaced vector while depensity dependent
          !flow use non-interlaced structure. The latter needs VecStrideScatter and 
          !Gather operation. To get higher efficiency, modify the depensity dependent
          !flow into interlaced structure.
              
          !Zero entries
          call VecZeroEntries(x_vec_loc, ierr)
          CHKERRQ(ierr)
          
          !Get a pointer to vector data when you need access to the array        
          call VecGetArrayF90(x_vec_loc, vecpointer, ierr)
          CHKERRQ(ierr)
          
          !Compute the function over the locally owned part of the grid 
          if(b_non_interlaced) then
            j = nngl/2
            do i = 1, j
              vecpointer(2*i-1) = x_array_loc(i)
              vecpointer(2*i) = x_array_loc(i+j)
            end do
          else
            do i = 1, nngl
              vecpointer(i) = x_array_loc(i)
            end do
          end if
          
          !Restore the vector when you no longer need access to the array
          call VecRestoreArrayF90(x_vec_loc,vecpointer,ierr)
          CHKERRQ(ierr)
          
          !Insert values into global vector
          call DMLocalToGlobalBegin(da,x_vec_loc,INSERT_VALUES,      &
                                    x_vec_gbl,ierr)
          CHKERRQ(ierr)

          !By placing code between these two statements, computations can be
          !done while messages are in transition.
          call DMLocalToGlobalEnd(da,x_vec_loc,INSERT_VALUES,        &
                                  x_vec_gbl,ierr)
          CHKERRQ(ierr)
          
          info_debug = 0
          if(info_debug > 0) then
              write(*,'(a,1x,i4)')                                     &
                    "Initial guess of flow finished from rank ", rank
          end if
          
          return
    
      end subroutine form_initial_guess
      
      !> Evaluate nonlinear function at new location
      subroutine compute_function(rank,da,x_array_loc,x_vec_loc,       &
                      x_vec_gbl,nngl,row_idx_l2pg,col_idx_l2pg,        &
                      b_non_interlaced)
          
          implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscdmda.h90>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscis.h>
#include <finclude/petscvec.h>
#include <finclude/petscvec.h90>
#include <finclude/petscmat.h>
#include <finclude/petscpc.h>
#include <finclude/petscksp.h>
#include <finclude/petscdmda.h>
#include <finclude/petscdmda.h90>
#endif

          PetscInt :: rank
          DM :: da 
          PetscReal, allocatable :: x_array_loc(:)
          Vec :: x_vec_loc
          Vec :: x_vec_gbl
          PetscInt :: nngl
          PetscInt, allocatable :: row_idx_l2pg(:)
          PetscInt, allocatable :: col_idx_l2pg(:)
          PetscBool :: b_non_interlaced

          
          PetscInt :: info_debug
          PetscInt :: i, j
          PetscErrorCode :: ierr
          PetscScalar, pointer :: vecpointer(:)
          
          
          !!Scatter ghost points to local vector, using the 2-step process
          !!DMGlobalToLocalBegin(), DMGlobalToLocalEnd().          
          !call DMGlobalToLocalBegin(da,x_vec_gbl,INSERT_VALUES,x_vec_loc,ierr)          
          !!By placing code between these two statements, computations can be
          !!done while messages are in transition.
          !call DMGlobalToLocalEnd(da,x_vec_gbl,INSERT_VALUES,x_vec_loc,ierr)   
          
          !By default, PETSc use interlaced vector.
          !The reactive transport use interlaced vector while depensity dependent
          !flow use non-interlaced structure. The latter needs VecStrideScatter and 
          !Gather operation. To get higher efficiency, modify the depensity dependent
          !flow into interlaced structure.
              
          !Zero entries
          call VecZeroEntries(x_vec_loc, ierr)
          CHKERRQ(ierr)
          
          !Get a pointer to vector data when you need access to the array        
          call VecGetArrayF90(x_vec_loc, vecpointer, ierr)
          CHKERRQ(ierr)
          
          !Compute the function over the locally owned part of the grid 
          if(b_non_interlaced) then
            j = nngl/2
            do i = 1, j
              vecpointer(2*i-1) = x_array_loc(i)
              vecpointer(2*i) = x_array_loc(i+j)
            end do
          else
            do i = 1, nngl
              vecpointer(i) = x_array_loc(i)
            end do
          end if
          
          !Restore the vector when you no longer need access to the array
          call VecRestoreArrayF90(x_vec_loc,vecpointer,ierr)
          CHKERRQ(ierr)
          
          !Insert values into global vector
          call DMLocalToGlobalBegin(da,x_vec_loc,INSERT_VALUES,      &
                                    x_vec_gbl,ierr)
          CHKERRQ(ierr)

          !By placing code between these two statements, computations can be
          !done while messages are in transition.
          call DMLocalToGlobalEnd(da,x_vec_loc,INSERT_VALUES,        &
                                  x_vec_gbl,ierr)
          CHKERRQ(ierr)
         
          info_debug = 0
          if(info_debug > 0) then
              write(*,'(a,1x,i4)')                                     &
                    "compute function of flow finished from rank ", rank
          end if
          
          return
      
      end subroutine
      
      !> Compute the Jacobian matrix
      subroutine compute_jacobian(rank,da,mat_gbl,a_loc,ia_loc,        &
                         ja_loc,nngl,row_idx_l2pg,col_idx_l2pg,        &
                         b_non_interlaced)
          
          implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscdmda.h90>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscis.h>
#include <finclude/petscvec.h>
#include <finclude/petscvec.h90>
#include <finclude/petscmat.h>
#include <finclude/petscpc.h>
#include <finclude/petscksp.h>
#include <finclude/petscdmda.h>
#include <finclude/petscdmda.h90>
#endif

          PetscInt :: rank
          PetscInt :: nngl
          DM :: da 
          Mat :: mat_gbl
          PetscReal, allocatable :: a_loc(:)
          PetscInt, allocatable :: ia_loc(:)
          PetscInt, allocatable :: ja_loc(:)
          PetscInt, allocatable :: row_idx_l2pg(:)
          PetscInt, allocatable :: col_idx_l2pg(:)
          PetscBool :: b_non_interlaced

          PetscInt :: i, j, k, istart, iend
          PetscInt :: info_debug
          PetscErrorCode :: ierr
          
          !Set matrix elements
          ! - Each processor needs to insert only elements that it owns
          !   locally (but any non-local elements will be sent to the
          !   appropriate processor during matrix assembly).
          ! - Always specify global row and columns of matrix entries.
          ! - Note that MatSetValues() uses 0-based row and column numbers
          !   in Fortran as well as in C.        
          call MatZeroEntries(mat_gbl, ierr)
          CHKERRQ(ierr)
         
          do i = 1, nngl
            j = row_idx_l2pg(i)
            if(j < 0) then
                cycle
            end if
            istart = ia_loc(i)
            iend = ia_loc(i+1)-1 
            
            call MatSetValues(mat_gbl,1,j-1,                           &
                       iend-istart+1,col_idx_l2pg(istart:iend)-1,      &
                       a_loc(istart:iend),Insert_Values, ierr)
            CHKERRQ(ierr)

          end do

          !call PetscFinalize(ierr)
          !CHKERRQ(ierr)
          !stop
          
          !  Assemble matrix, using the 2-step process:
          !    MatAssemblyBegin(), MatAssemblyEnd().
          call MatAssemblyBegin(mat_gbl,MAT_FINAL_ASSEMBLY,ierr)
          CHKERRQ(ierr)
          !  By placing code between these two statements, computations can be
          !  done while messages are in transition.
          call MatAssemblyEnd(mat_gbl,MAT_FINAL_ASSEMBLY,ierr)
          CHKERRQ(ierr)
          
          info_debug = 0
          if(info_debug > 0) then
              write(*,'(a,1x,i4)')                                     &
                    "compute jacobian of flow finished from rank ", rank
          end if
      
          return
          
      end subroutine compute_jacobian
    

! ---------------------------------------------------------------------
!
!  FormFunction - Evaluates nonlinear function, F(x).
!
!  Input Parameters:
!  snes - the SNES context
!  X - input vector
!  dummy - optional user-defined context, as set by SNESSetFunction()
!          (not used here)
!
!  Output Parameter:
!  F - function vector
!
!  Notes:
!  This routine serves as a wrapper for the lower-level routine
!  "FormFunctionLocal", where the actual computations are
!  done using the standard Fortran style of treating the local
!  vector data as a multidimensional array over the local mesh.
!  This routine merely handles ghost point scatters and accesses
!  the local vector data via VecGetArrayF90() and VecRestoreArrayF90().
!
!  Example of FormFunction
      subroutine FormFunction(snes,X,F,user,ierr)
      
      use solver_snes_common, only : userctx
      
      implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscsnes.h>
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscsnes.h90>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscdmda.h>
#include <finclude/petscis.h>
#include <finclude/petscmat.h>
#include <finclude/petscksp.h>
#include <finclude/petscpc.h>
#include <finclude/petscsnes.h>
#include <finclude/petscvec.h90>
#include <finclude/petscsnes.h90>
#endif

!  Input/output variables:
      SNES           snes
      Vec            X,F
      PetscErrorCode ierr
      type (userctx) user
      DM             da

!  Declarations for use with local arrays:
      PetscScalar,pointer :: lx_v(:),lf_v(:)
      Vec            localX

!  Scatter ghost points to local vector, using the 2-step process
!     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
!  By placing code between these two statements, computations can
!  be done while messages are in transition.
      call SNESGetDM(snes,da,ierr)
      CHKERRQ(ierr)
      call DMGetLocalVector(da,localX,ierr)
      CHKERRQ(ierr)
      call DMGlobalToLocalBegin(da,X,INSERT_VALUES,                     &
                                localX,ierr)
      CHKERRQ(ierr)
      call DMGlobalToLocalEnd(da,X,INSERT_VALUES,localX,ierr)
      CHKERRQ(ierr)

!  Get a pointer to vector data.
!    - For default PETSc vectors, VecGetArray90() returns a pointer to
!      the data array. Otherwise, the routine is implementation dependent.
!    - You MUST call VecRestoreArrayF90() when you no longer need access to
!      the array.
!    - Note that the interface to VecGetArrayF90() differs from VecGetArray(),
!      and is useable from Fortran-90 Only.

      call VecGetArrayF90(localX,lx_v,ierr)
      CHKERRQ(ierr)
      call VecGetArrayF90(F,lf_v,ierr)
      CHKERRQ(ierr)

!  Compute function over the locally owned part of the grid
      call FormFunctionLocal(lx_v,lf_v,user,ierr)
      CHKERRQ(ierr)

!  Restore vectors
      call VecRestoreArrayF90(localX,lx_v,ierr)
      CHKERRQ(ierr)
      call VecRestoreArrayF90(F,lf_v,ierr)
      CHKERRQ(ierr)

!  Insert values into global vector

      call DMRestoreLocalVector(da,localX,ierr)
      CHKERRQ(ierr)
      call PetscLogFlops(11.0d0*user%yl*user%xl,ierr)
      CHKERRQ(ierr)

!      call VecView(X,PETSC_VIEWER_STDOUT_WORLD,ierr)
!      CHKERRQ(ierr)
!      call VecView(F,PETSC_VIEWER_STDOUT_WORLD,ierr)
!      CHKERRQ(ierr)
      return
      end subroutine formfunction      
  
! ---------------------------------------------------------------------
!
!  FormInitialGuess - Forms initial approximation.
!
!  Input Parameters:
!  X - vector
!
!  Output Parameter:
!  X - vector
!
!  Notes:
!  This routine serves as a wrapper for the lower-level routine
!  "InitialGuessLocal", where the actual computations are
!  done using the standard Fortran style of treating the local
!  vector data as a multidimensional array over the local mesh.
!  This routine merely handles ghost point scatters and accesses
!  the local vector data via VecGetArrayF90() and VecRestoreArrayF90().
!
!  Example of FormInitialGuess
      subroutine FormInitialGuess(snes,X,ierr)
      
      use solver_snes_common, only : userctx
      
      implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscsnes.h>
#elif PETSC
#include <finclude/petscvec.h90>
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscdmda.h>
#include <finclude/petscis.h>
#include <finclude/petscmat.h>
#include <finclude/petscksp.h>
#include <finclude/petscpc.h>
#include <finclude/petscsnes.h>
#endif

!  Input/output variables:
      SNES           snes
      type(userctx), pointer:: puser => null()
      Vec            X
      PetscErrorCode ierr
      DM             da

!  Declarations for use with local arrays:
      PetscScalar,pointer :: lx_v(:)
      Vec               localX

      ierr = 0
      call SNESGetDM(snes,da,ierr)
      CHKERRQ(ierr)
      call SNESGetApplicationContext(snes,puser,ierr)
      CHKERRQ(ierr)
!  Get a pointer to vector data.
!    - For default PETSc vectors, VecGetArray90() returns a pointer to
!      the data array. Otherwise, the routine is implementation dependent.
!    - You MUST call VecRestoreArrayF90() when you no longer need access to
!      the array.
!    - Note that the interface to VecGetArrayF90() differs from VecGetArray(),
!      and is useable from Fortran-90 Only.

      call DMGetLocalVector(da,localX,ierr)
      CHKERRQ(ierr)
      call VecGetArrayF90(localX,lx_v,ierr)
      CHKERRQ(ierr)

!  Compute initial guess over the locally owned part of the grid
      call InitialGuessLocal(puser,lx_v,ierr)
      CHKERRQ(ierr)

!  Restore vector
      call VecRestoreArrayF90(localX,lx_v,ierr)
      CHKERRQ(ierr)

!  Insert values into global vector
      call DMLocalToGlobalBegin(da,localX,INSERT_VALUES,X,ierr)
      CHKERRQ(ierr)
      call DMLocalToGlobalEnd(da,localX,INSERT_VALUES,X,ierr)
      CHKERRQ(ierr)
      call DMRestoreLocalVector(da,localX,ierr)
      CHKERRQ(ierr)

      return
      end subroutine

! ---------------------------------------------------------------------
!
!  InitialGuessLocal - Computes initial approximation, called by
!  the higher level routine FormInitialGuess().
!
!  Input Parameter:
!  x - local vector data
!
!  Output Parameters:
!  x - local vector data
!  ierr - error code
!
!  Notes:
!  This routine uses standard Fortran-style computations over a 2-dim array.
!
!  Example of InitialGuessLocal
      subroutine InitialGuessLocal(user,x,ierr)
      
      use solver_snes_common, only : userctx
      
      implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscsnes.h>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscdmda.h>
#include <finclude/petscis.h>
#include <finclude/petscmat.h>
#include <finclude/petscksp.h>
#include <finclude/petscpc.h>
#include <finclude/petscsnes.h>
#endif

!  Input/output variables:
      type (userctx)         user
      PetscScalar  x(user%gxs:user%gxe,                                 &
     &              user%gys:user%gye)
      PetscErrorCode ierr

!  Local variables:
      PetscInt  i,j
      PetscScalar   temp1,temp,hx,hy
      PetscScalar   one

!  Set parameters

      ierr   = 0
      one    = 1.0
      hx     = one/(dble(user%nvxgbl-1))
      hy     = one/(dble(user%nvygbl-1))
      temp1  = user%lambda/(user%lambda + one)

      do 20 j=user%ys,user%ye
         temp = dble(min(j-1,user%nvygbl-j))*hy
         do 10 i=user%xs,user%xe
            if (i .eq. 1 .or. j .eq. 1                                  &
     &             .or. i .eq. user%nvxgbl .or. j .eq. user%nvygbl) then
              x(i,j) = 0.0
            else
              x(i,j) = temp1 *                                          &
     &          sqrt(min(dble(min(i-1,user%nvxgbl-i)*hx),dble(temp)))
            endif
 10      continue
 20   continue

      return
      end subroutine

! ---------------------------------------------------------------------
!
!  FormFunctionLocal - Computes nonlinear function, called by
!  the higher level routine FormFunction().
!
!  Input Parameter:
!  x - local vector data
!
!  Output Parameters:
!  f - local vector data, f(x)
!  ierr - error code
!
!  Notes:
!  This routine uses standard Fortran-style computations over a 2-dim array.
!
!  Example of FormFunctionLocal
      subroutine FormFunctionLocal(x,f,user,ierr)
      
      use solver_snes_common, only : userctx
      
      implicit none

!  Input/output variables:
      type (userctx) user
      PetscScalar  x(user%gxs:user%gxe,                                         &
     &              user%gys:user%gye)
      PetscScalar  f(user%xs:user%xe,                                           &
     &              user%ys:user%ye)
      PetscErrorCode ierr

!  Local variables:
      PetscScalar two,one,hx,hy,hxdhy,hydhx,sc
      PetscScalar u,uxx,uyy
      PetscInt  i,j

      one    = 1.0
      two    = 2.0
      hx     = one/dble(user%nvxgbl-1)
      hy     = one/dble(user%nvygbl-1)
      sc     = hx*hy*user%lambda
      hxdhy  = hx/hy
      hydhx  = hy/hx

!  Compute function over the locally owned part of the grid

      do 20 j=user%ys,user%ye
         do 10 i=user%xs,user%xe
            if (i .eq. 1 .or. j .eq. 1                                  &
     &             .or. i .eq. user%nvxgbl .or. j .eq. user%nvygbl) then
               f(i,j) = x(i,j)
            else
               u = x(i,j)
               uxx = hydhx * (two*u                                     &
     &                - x(i-1,j) - x(i+1,j))
               uyy = hxdhy * (two*u - x(i,j-1) - x(i,j+1))
               f(i,j) = uxx + uyy - sc*exp(u)
            endif
 10      continue
 20   continue

      return
      end subroutine

! ---------------------------------------------------------------------
!
!  FormJacobian - Evaluates Jacobian matrix.
!
!  Input Parameters:
!  snes     - the SNES context
!  x        - input vector
!  dummy    - optional user-defined context, as set by SNESSetJacobian()
!             (not used here)
!
!  Output Parameters:
!  jac      - Jacobian matrix
!  jac_prec - optionally different preconditioning matrix (not used here)
!  flag     - flag indicating matrix structure
!
!  Notes:
!  This routine serves as a wrapper for the lower-level routine
!  "FormJacobianLocal", where the actual computations are
!  done using the standard Fortran style of treating the local
!  vector data as a multidimensional array over the local mesh.
!  This routine merely accesses the local vector data via
!  VecGetArrayF90() and VecRestoreArrayF90().
!
!  Notes:
!  Due to grid point reordering with DMDAs, we must always work
!  with the local grid points, and then transform them to the new
!  global numbering with the "ltog" mapping (via DMDAGetGlobalIndicesF90()).
!  We cannot work directly with the global numbers for the original
!  uniprocessor grid!
!
!  Two methods are available for imposing this transformation
!  when setting matrix entries:
!    (A) MatSetValuesLocal(), using the local ordering (including
!        ghost points!)
!        - Use DMDAGetGlobalIndicesF90() to extract the local-to-global map
!        - Associate this map with the matrix by calling
!          MatSetLocalToGlobalMapping() once
!        - Set matrix entries using the local ordering
!          by calling MatSetValuesLocal()
!    (B) MatSetValues(), using the global ordering
!        - Use DMDAGetGlobalIndicesF90() to extract the local-to-global map
!        - Then apply this map explicitly yourself
!        - Set matrix entries using the global ordering by calling
!          MatSetValues()
!  Option (A) seems cleaner/easier in many cases, and is the procedure
!  used in this example.
!
!  Example of FormJacobian
      subroutine FormJacobian(snes,X,jac,jac_prec,flag,user,ierr)
      
      use solver_snes_common, only : userctx
      
      implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscsnes.h>
#include <petsc/finclude/petscvec.h90>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscdmda.h>
#include <finclude/petscis.h>
#include <finclude/petscmat.h>
#include <finclude/petscksp.h>
#include <finclude/petscpc.h>
#include <finclude/petscsnes.h>
#include <finclude/petscvec.h90>
#endif

!  Input/output variables:
      SNES         snes
      Vec          X
      Mat          jac,jac_prec
      MatStructure flag
      type(userctx)  user
      PetscErrorCode ierr
      DM             da

!  Declarations for use with local arrays:
      PetscScalar,pointer :: lx_v(:)
      Vec            localX

!  Scatter ghost points to local vector, using the 2-step process
!     DMGlobalToLocalBegin(), DMGlobalToLocalEnd()
!  Computations can be done while messages are in transition,
!  by placing code between these two statements.

      call SNESGetDM(snes,da,ierr)
      CHKERRQ(ierr)
      call DMGetLocalVector(da,localX,ierr)
      CHKERRQ(ierr)
      call DMGlobalToLocalBegin(da,X,INSERT_VALUES,localX,              &
                                ierr)
      CHKERRQ(ierr)
      call DMGlobalToLocalEnd(da,X,INSERT_VALUES,localX,ierr)
      CHKERRQ(ierr)

!  Get a pointer to vector data
      call VecGetArrayF90(localX,lx_v,ierr)
      CHKERRQ(ierr)

!  Compute entries for the locally owned part of the Jacobian preconditioner.
      call FormJacobianLocal(lx_v,jac_prec,user,ierr)
      CHKERRQ(ierr)

!  Assemble matrix, using the 2-step process:
!     MatAssemblyBegin(), MatAssemblyEnd()
!  Computations can be done while messages are in transition,
!  by placing code between these two statements.

      call MatAssemblyBegin(jac,MAT_FINAL_ASSEMBLY,ierr)
      CHKERRQ(ierr)
      if (jac .ne. jac_prec) then
         call MatAssemblyBegin(jac_prec,MAT_FINAL_ASSEMBLY,ierr)
         CHKERRQ(ierr)
      endif
      call VecRestoreArrayF90(localX,lx_v,ierr)
      CHKERRQ(ierr)
      call DMRestoreLocalVector(da,localX,ierr)
      CHKERRQ(ierr)
      call MatAssemblyEnd(jac,MAT_FINAL_ASSEMBLY,ierr)
      CHKERRQ(ierr)
      if (jac .ne. jac_prec) then
        call MatAssemblyEnd(jac_prec,MAT_FINAL_ASSEMBLY,ierr)
        CHKERRQ(ierr)
      endif

!  Set flag to indicate that the Jacobian matrix retains an identical
!  nonzero structure throughout all nonlinear iterations (although the
!  values of the entries change). Thus, we can save some work in setting
!  up the preconditioner (e.g., no need to redo symbolic factorization for
!  ILU/ICC preconditioners).
!   - If the nonzero structure of the matrix is different during
!     successive linear solves, then the flag DIFFERENT_NONZERO_PATTERN
!     must be used instead.  If you are unsure whether the matrix
!     structure has changed or not, use the flag DIFFERENT_NONZERO_PATTERN.
!   - Caution:  If you specify SAME_NONZERO_PATTERN, PETSc
!     believes your assertion and does not check the structure
!     of the matrix.  If you erroneously claim that the structure
!     is the same when it actually is not, the new preconditioner
!     will not function correctly.  Thus, use this optimization
!     feature with caution!

      flag = SAME_NONZERO_PATTERN

!  Tell the matrix we will never add a new nonzero location to the
!  matrix. If we do it will generate an error.

      call MatSetOption(jac,MAT_NEW_NONZERO_LOCATION_ERR,PETSC_TRUE,      &
                        ierr)
      CHKERRQ(ierr)
      return
      end subroutine

! ---------------------------------------------------------------------
!
!  FormJacobianLocal - Computes Jacobian preconditioner matrix,
!  called by the higher level routine FormJacobian().
!
!  Input Parameters:
!  x        - local vector data
!
!  Output Parameters:
!  jac_prec - Jacobian preconditioner matrix
!  ierr     - error code
!
!  Notes:
!  This routine uses standard Fortran-style computations over a 2-dim array.
!
!  Notes:
!  Due to grid point reordering with DMDAs, we must always work
!  with the local grid points, and then transform them to the new
!  global numbering with the "ltog" mapping (via DMDAGetGlobalIndicesF90()).
!  We cannot work directly with the global numbers for the original
!  uniprocessor grid!
!
!  Two methods are available for imposing this transformation
!  when setting matrix entries:
!    (A) MatSetValuesLocal(), using the local ordering (including
!        ghost points!)
!        - Use DMDAGetGlobalIndicesF90() to extract the local-to-global map
!        - Associate this map with the matrix by calling
!          MatSetLocalToGlobalMapping() once
!        - Set matrix entries using the local ordering
!          by calling MatSetValuesLocal()
!    (B) MatSetValues(), using the global ordering
!        - Use DMDAGetGlobalIndicesF90() to extract the local-to-global map
!        - Then apply this map explicitly yourself
!        - Set matrix entries using the global ordering by calling
!          MatSetValues()
!  Option (A) seems cleaner/easier in many cases, and is the procedure
!  used in this example.
!
!  Example of FormJacobianLocal
      subroutine FormJacobianLocal(x,jac_prec,user,ierr)
      
      use solver_snes_common, only : userctx
      
      implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscis.h>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscsnes.h>
#elif PETSC
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscdmda.h>
#include <finclude/petscis.h>
#include <finclude/petscmat.h>
#include <finclude/petscksp.h>
#include <finclude/petscpc.h>
#include <finclude/petscsnes.h>
#endif

!  Input/output variables:
      type (userctx) user
      PetscScalar    x(user%gxs:user%gxe,                                      &
     &               user%gys:user%gye)
      Mat            jac_prec
      PetscErrorCode ierr

!  Local variables:
      PetscInt    row,col(5),i,j
      PetscInt    ione,ifive
      PetscScalar two,one,hx,hy,hxdhy
      PetscScalar hydhx,sc,v(5)

!  Set parameters
      ione   = 1
      ifive  = 5
      one    = 1.0
      two    = 2.0
      hx     = one/dble(user%nvxgbl-1)
      hy     = one/dble(user%nvygbl-1)
      sc     = hx*hy
      hxdhy  = hx/hy
      hydhx  = hy/hx

!  Compute entries for the locally owned part of the Jacobian.
!   - Currently, all PETSc parallel matrix formats are partitioned by
!     contiguous chunks of rows across the processors.
!   - Each processor needs to insert only elements that it owns
!     locally (but any non-local elements will be sent to the
!     appropriate processor during matrix assembly).
!   - Here, we set all entries for a particular row at once.
!   - We can set matrix entries either using either
!     MatSetValuesLocal() or MatSetValues(), as discussed above.
!   - Note that MatSetValues() uses 0-based row and column numbers
!     in Fortran as well as in C.

      do 20 j=user%ys,user%ye
         row = (j - user%gys)*user%gxl + user%xs - user%gxs - 1
         do 10 i=user%xs,user%xe
            row = row + 1
!           boundary points
            if (i .eq. 1 .or. j .eq. 1                                  &
     &             .or. i .eq. user%nvxgbl .or. j .eq. user%nvygbl) then
               col(1) = row
               v(1)   = one
               call MatSetValuesLocal(jac_prec,ione,row,ione,col,v,     &
     &                           INSERT_VALUES,ierr)
               CHKERRQ(ierr)
!           interior grid points
            else
               v(1) = -hxdhy
               v(2) = -hydhx
               v(3) = two*(hydhx + hxdhy)                               &
     &                  - sc*user%lambda*exp(x(i,j))
               v(4) = -hydhx
               v(5) = -hxdhy
               col(1) = row - user%gxl
               col(2) = row - 1
               col(3) = row
               col(4) = row + 1
               col(5) = row + user%gxl
               call MatSetValuesLocal(jac_prec,ione,row,ifive,col,v,         &
     &                                INSERT_VALUES,ierr)
               CHKERRQ(ierr)
            endif
 10      continue
 20   continue

      return 
      end subroutine

end module solver_snes_function
    
#endif
