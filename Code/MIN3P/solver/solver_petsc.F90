!*****Revision Informations Automatically Generated by VisualSVN*****!
!---------------------------------------------------------------------
!> $ID:$
!> $Revision: 453 $
!> $Author: dsu $
!> $Date: 2017-02-21 19:54:05 +0100 (Tue, 21 Feb 2017) $
!> $URL: https://biot.eos.ubc.ca/svn/min3p_thcm/branches/fgerard_new/src/solver/solver_petsc.F90 $
!---------------------------------------------------------------------
!********************************************************************!

#ifdef PETSC

!> module: solver_petsc
!>
!> written by: Danyang Su
!>
!> module description:
!>
!> Note
!> This module is called by the master thread only. For the domain decomposition
!> method, it is deprecated, look into 'solver_snes...F90' for the new implementation.
!>
!> 
!> Module of PETSc solver for unsymmetric linear systems  
!>
!> Note: This module is not high efficient at present. 
!> See http://www.mcs.anl.gov/petsc/ for detail


module solver_petsc

    use gen, only: idbg, b_mpi_process_flag, rank, nprcs

    implicit none
#ifdef PETSC_V3_6_X
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscmat.h>
#include <petsc/finclude/petscpc.h>
#include <petsc/finclude/petscksp.h>
#include <petsc/finclude/petscis.h>
#else
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscvec.h90>
#include <finclude/petscmat.h>
#include <finclude/petscpc.h>
#include <finclude/petscksp.h>
#include <finclude/petscis.h> 
#endif

    !> Local variables
    integer :: info_debug

    !> Common parameters
    !PetscMPIInt, public     :: rank
    !PetscMPIInt, public     :: nprcs
    PetscMPIInt, public     :: tag      
    PetscErrorCode, public  :: ierrcode
    PetscScalar, pointer    :: vecpointer(:)
    PetscViewer, private    :: viewer
    PetscBool, private      :: b_initialized_flow
    PetscBool, private      :: b_initialized_react
    
    PetscReal, parameter :: neg_one = -1.0d0
    
    !> @param rtol the relative convergence tolerance (relative decrease in the residual norm) 
    PetscReal, public       :: rtol
    !> @param abstol the absolute convergence tolerance (absolute size of the residual norm) 
    PetscReal, public       :: abstol
    !> @param dtol  the divergence tolerance (amount residual can increase before KSPDefaultConverged() concludes that the method is diverging) 
    PetscReal, public       :: dtol
    !> @param maxits maximum number of iterations to use    
    PetscInt, public        :: maxits
    
  
    !> Parameters for flow 
    KSP, private :: ksp_flow
    PC, private  :: pc_flow
    Mat, private :: a_flow
    Vec, private :: b_flow
    Vec, private :: x_flow
    Vec, private :: u_flow
    
    !> @strKSPType PETSc KSP solver type, e.g, "KSPGMRES", "KSPBCGS"
    character(16), public   :: strKSPType_flow
    
    !> @strKSPNormType PETSc KSP Norm type, e.g., "KSP_NORM_NONE", "KSP_NORM_PRECONDITIONED"
    !>                                            "KSP_NORM_UNPRECONDITIONED", "KSP_NORM_NATURAL"
    character(32), public   :: strKSPNormType_flow
    
    !> @strPCType_flow PETSc preconditioner type in flow, e.g., "pcbjacobi", "pcilu"
    character(32), public   :: strPCType_flow
    
    !> @strKSPConvergenceType_flow PETSc convergence criteria type in flow, e.g., "kspdefault", "kspuserdefined"
    character(32), public   :: strKSPConvergenceType_flow
    
    !> @b_mykspconverged_flow
    PetscBool, private :: b_mykspconverged_flow
    
    !> @param rtol the relative convergence tolerance (relative decrease in the residual norm) 
    PetscReal, public       :: rtol_flow         !default 1.0E-5
    !> @param abstol the absolute convergence tolerance (absolute size of the residual norm) 
    PetscReal, public       :: abstol_flow       !default 1.0E-50
    !> @param dtol  the divergence tolerance (amount residual can increase before KSPDefaultConverged() concludes that the method is diverging) 
    PetscReal, public       :: dtol_flow         !default 1.0E5
    !> @param maxits maximum number of iterations to use    
    PetscInt, public        :: maxits_flow       !default 1.0E5
    !> @param residual norm, can be preconditioned residual norm or true residual norm
    PetscReal, public       :: rnorm_flow
    !> @param initial residual norm, can be preconditioned residual norm or true residual norm
    PetscReal, private      :: rnorm_init_flow
    
    PetscReal, allocatable, private :: a_flow_loc(:)     !local data, for temporary use
    PetscReal, allocatable, private :: b_flow_loc(:)     !local data, for temporary use
    PetscReal, allocatable, private :: x_flow_loc(:)     !local data, for temporary use
    
    PetscInt, allocatable, private :: ia_flow(:)
    PetscInt, allocatable, private :: ja_flow(:)
    
    PetscInt, private :: rcounts_nb_flow
    PetscInt, private :: rcounts_nnz_flow
    
    PetscInt, private :: istart_flow
    PetscInt, private :: iend_flow
    
    PetscInt, allocatable, private :: ranges_flow(:)
    
    PetscInt, allocatable, private :: displs_nb_flow(:)
    PetscInt, allocatable, private :: scounts_nb_flow(:)
    
    PetscInt, allocatable, private :: displs_nnz_flow(:)
    PetscInt, allocatable, private :: scounts_nnz_flow(:)
    
    PetscInt, allocatable, private :: ix_flow(:)
    
    logical, private :: bSetMatOption_flow
    
    
    !> Parameters for reactive transport
    KSP, private :: ksp_react
    PC, private  :: pc_react
    Mat, private :: a_react
    Vec, private :: b_react
    Vec, private :: x_react
    Vec, private :: u_react
    
    !> @strKSPType PETSc KSP solver type, e.g, "KSPGMRES", "KSPBCGS"
    character(16), public   :: strKSPType_react
    
    !> @strKSPNormType PETSc KSP Norm type, e.g., "KSP_NORM_NONE", "KSP_NORM_PRECONDITIONED"
    !>                                            "KSP_NORM_UNPRECONDITIONED", "KSP_NORM_NATURAL"
    character(32), public   :: strKSPNormType_react
    
    !> @strPCType_react PETSc preconditioner type in reactive transport, e.g., "pcbjacobi", "pcilu"
    character(32), public   :: strPCType_react
    
    !> @strKSPConvergenceType_react PETSc convergence criteria type in reactive transport, e.g., "kspdefault", "kspuserdefined"
    character(32), public   :: strKSPConvergenceType_react
    
    !> @b_mykspconverged_react
    PetscBool, private :: b_mykspconverged_react
    
    !> @param rtol the relative convergence tolerance (relative decrease in the residual norm) 
    PetscReal, public       :: rtol_react         !default 1.0E-5
    !> @param abstol the absolute convergence tolerance (absolute size of the residual norm) 
    PetscReal, public       :: abstol_react       !default 1.0E-50
    !> @param dtol  the divergence tolerance (amount residual can increase before KSPDefaultConverged() concludes that the method is diverging) 
    PetscReal, public       :: dtol_react         !default 1.0E5
    !> @param maxits maximum number of iterations to use    
    PetscInt, public        :: maxits_react       !default 1.0E5
    !> @param residual norm, can be preconditioned residual norm or true residual norm
    PetscReal, public       :: rnorm_react
    !> @param initial residual norm, can be preconditioned residual norm or true residual norm
    PetscReal, private      :: rnorm_init_react
    
    PetscReal, allocatable, private :: a_react_loc(:)     !local data, for temporary use
    PetscReal, allocatable, private :: b_react_loc(:)     !local data, for temporary use
    PetscReal, allocatable, private :: x_react_loc(:)     !local data, for temporary use
    
    PetscInt, allocatable, private :: ia_react(:)
    PetscInt, allocatable, private :: ja_react(:)
    
    PetscInt, private :: rcounts_nb_react
    PetscInt, private :: rcounts_nnz_react
    
    PetscInt, private :: istart_react
    PetscInt, private :: iend_react
    
    PetscInt, allocatable, private :: ranges_react(:)
    
    PetscInt, allocatable, private :: displs_nb_react(:)
    PetscInt, allocatable, private :: scounts_nb_react(:)
    
    PetscInt, allocatable, private :: displs_nnz_react(:)
    PetscInt, allocatable, private :: scounts_nnz_react(:)
    
    PetscInt, allocatable, private :: ix_react(:)
    
    logical, private :: bSetMatOption_react  
    
   
    public petsc_set_initialize, petsc_set_barrier,                  &
           petsc_solver_create_flow, petsc_solver_ksp_flow,          &
           petsc_solver_create_react, petsc_solver_ksp_react,        &
           petsc_solver_release, petsc_set_finalize
           
    
    contains
    
    !>
    !>petsc_set_initialize
    !>
    subroutine petsc_set_initialize
    
        implicit none
        
        call PetscInitialize(Petsc_Null_Character,ierrcode)
        CHKERRQ(ierrcode)
        call MPI_Comm_rank(Petsc_Comm_World,rank,ierrcode)
        CHKERRQ(ierrcode)
        call MPI_Comm_size(Petsc_Comm_World,nprcs,ierrcode)
        CHKERRQ(ierrcode)
        
        b_initialized_flow = .false.
        b_initialized_react = .false.
        
        b_mpi_process_flag = .false.
    
    end subroutine petsc_set_initialize
    
    !>
    !> petsc_set_barrier
    !> Set barrier
    !>
    subroutine petsc_set_barrier
    
        implicit none
#ifdef MPI        
        call MPI_BARRIER(Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#endif
    
    end subroutine petsc_set_barrier 
    
    !>
    !> mykspconverged_flow
    !> This is a user-defined routine for testing
    !> convergence of the KSP iterative solvers.
    !>  Input Parameters:
    !>    ksp   - iterative context
    !>    n     - iteration number
    !>    rnorm - 2-norm (preconditioned) residual value (may be estimated)
    !>    dummy - optional user-defined monitor context (unused here)
    subroutine mykspconverged_flow(ksp_flow,n,rnorm,flag,dummy,ierr)
    
       implicit none

       KSP                :: ksp_flow
       Vec                :: x_flow
       PetscErrorCode     :: ierr
       PetscInt           :: n,dummy
       KSPConvergedReason :: flag
       PetscReal          :: rnorm
       PetscReal          :: rnorm_relative
#ifdef DEBUG
      PetscMPIInt         :: rank
#endif
       
       if(n == 0) then
           rnorm_init_flow = rnorm
       end if
       
       rnorm_relative = rnorm / rnorm_init_flow
       
        !Get true residual norm
        !This operation is expensive, use -ksp_norm_type unpreconditioned instead
        if(n > 0) then
            call KSPBuildSolution(ksp_flow,PETSC_NULL_OBJECT,x_flow,ierrcode)
            CHKERRQ(ierrcode)
            call KSPGetRhs(ksp_flow, b_flow, ierrcode)
            CHKERRQ(ierrcode)
            call KSPGetOperators(ksp_flow,a_flow,PETSC_NULL_OBJECT ,PETSC_NULL_OBJECT, ierrcode)
            CHKERRQ(ierrcode)
            call MatMult(a_flow, x_flow, u_flow, ierrcode)
            CHKERRQ(ierrcode)
            call VecAXPY(u_flow, neg_one, b_flow, ierrcode)
            CHKERRQ(ierrcode)
            call VecNormBegin(u_flow, Norm_2, rnorm_flow, ierrcode)
            CHKERRQ(ierrcode)
            call VecNormEnd(u_flow, Norm_2, rnorm_flow, ierrcode)
            CHKERRQ(ierrcode)
        else
            rnorm_flow = 1.0d30
        end if

       if (n > 0 .and. (rnorm_flow < abstol_flow .or. rnorm_relative < rtol_flow)) then
         flag = 1
#ifdef DEBUG
         call MPI_Comm_rank(Petsc_Comm_World,rank,ierr)
         CHKERRQ(ierr)
#ifdef MPI_SHARED
         if(rank == 0) then
#endif
             if(rnorm_flow < abstol_flow .and. rnorm_relative < rtol_flow) then
                 write(*,'(2(1x, a, 1x, e15.7))') "KSP converged due to abstol", rnorm_flow, "and rstol", rnorm_relative
             else if (rnorm_flow < abstol_flow) then
                 write(*,'(1x, a, 1x, e15.7)') "KSP converged due to abstol", rnorm_flow
             else if (rnorm_relative < rtol_flow) then
                 write(*,'(1x, a, 1x, e15.7)') "KSP converged due to rstol", rnorm_relative
             end if
#ifdef MPI_SHARED
         end if
#endif
#endif
       else
         flag = 0
       endif
       
       ierr = 0

    end subroutine mykspconverged_flow
    
    
    !>
    !> mykspconverged_react
    !> This is a user-defined routine for testing
    !> convergence of the KSP iterative solvers.
    !>  Input Parameters:
    !>    ksp   - iterative context
    !>    n     - iteration number
    !>    rnorm - 2-norm (preconditioned) residual value (may be estimated)
    !>    dummy - optional user-defined monitor context (unused here)
    subroutine mykspconverged_react(ksp_react,n,rnorm,flag,dummy,ierr)
    
       implicit none

       KSP                :: ksp_react
       Vec                :: x_react
       PetscErrorCode     :: ierr
       PetscInt           :: n,dummy
       KSPConvergedReason :: flag
       PetscReal          :: rnorm
       PetscReal          :: rnorm_relative
#ifdef DEBUG
      PetscMPIInt         :: rank
#endif
       
       if(n == 0) then
           rnorm_init_react = rnorm
       end if
       
       rnorm_relative = rnorm / rnorm_init_react
       
        !Get true residual norm
        !This operation is expensive, use -ksp_norm_type unpreconditioned instead
        if(n > 0) then
            call KSPBuildSolution(ksp_react,PETSC_NULL_OBJECT,x_react,ierrcode)
            CHKERRQ(ierrcode)
            call KSPGetRhs(ksp_react, b_react, ierrcode)
            CHKERRQ(ierrcode)
            call KSPGetOperators(ksp_react,a_react,PETSC_NULL_OBJECT ,PETSC_NULL_OBJECT, ierrcode)
            CHKERRQ(ierrcode)
            call MatMult(a_react, x_react, u_react, ierrcode)
            CHKERRQ(ierrcode)
            call VecAXPY(u_react, neg_one, b_react, ierrcode)
            CHKERRQ(ierrcode)
            call VecNormBegin(u_react, Norm_2, rnorm_react, ierrcode)
            CHKERRQ(ierrcode)
            call VecNormEnd(u_react, Norm_2, rnorm_react, ierrcode)
            CHKERRQ(ierrcode)
        else
            rnorm_react = 1.0d30
        end if

       if (n > 0 .and. (rnorm_react < abstol_react .or. rnorm_relative < rtol_react)) then
         flag = 1
#ifdef DEBUG
         call MPI_Comm_rank(Petsc_Comm_World,rank,ierr)
         CHKERRQ(ierr)
#ifdef MPI_SHARED
         if(rank == 0) then
#endif
             if(rnorm_react < abstol_react .and. rnorm_relative < rtol_react) then
                 write(*,'(2(1x, a, 1x, e15.7))') "KSP converged due to abstol", rnorm_react, "and rstol", rnorm_relative
             else if (rnorm_react < abstol_react) then
                 write(*,'(1x, a, 1x, e15.7)') "KSP converged due to abstol", rnorm_react
             else if (rnorm_relative < rtol_react) then
                 write(*,'(1x, a, 1x, e15.7)') "KSP converged due to rstol", rnorm_relative
             end if
#ifdef MPI_SHARED
         end if
#endif
#endif
       else
         flag = 0
       endif
       
       ierr = 0

    end subroutine mykspconverged_react
  
    !>
    !> petsc_solver_create_flow(nb, ia, ja)
    !> Create PETSc solver space for flow problem
    !> @nb_in size of right hand side 
    !> @ia_in structure of sparse matrix ia
    !> @ja_in structure of sparse matrix ja
    !>
    !> Sample calling:
    !>   if(varsat_flow) then
    !>       if(density_dependence) then
    !>           if(heat_transport) then
    !>               call petsc_solver_create_flow(2*nn, iaglob, jaglob)
    !>           else
    !>               call petsc_solver_create_flow(nn, iavs, javs)
    !>           end if
    !>       else
    !>           call petsc_solver_create_flow(nn, iavs, javs)
    !>       end if
    !>   end if
    subroutine petsc_solver_create_flow(nb_in, ia_in, ja_in)
    
        implicit none
        
        PetscInt, intent(in) :: nb_in
        PetscInt, allocatable, intent(in) :: ia_in(:)
        PetscInt, allocatable, intent(in) :: ja_in(:)
        
        !local variable
        integer*4 :: i, j, k
        PetscInt, allocatable :: d_nnz(:)
        PetscInt, allocatable :: o_nnz(:)
        
        info_debug = 1        
        
        allocate(ranges_flow(nprcs+1))        
        allocate(displs_nb_flow(nprcs))        
        allocate(scounts_nb_flow(nprcs))        
        

        allocate(displs_nnz_flow(nprcs))
        allocate(scounts_nnz_flow(nprcs))
        
        !Broadcast nb_in
#ifdef MPI
        call MPI_BCAST (nb_in, 1, MPI_INTEGER4, 0, Petsc_Comm_World, &
                        ierrcode)
        CHKERRQ(ierrcode)
#endif
        
        !Allocate space for RHS b
        call VecCreateMPI(Petsc_Comm_World, Petsc_Decide, nb_in,     &
                          b_flow, ierrcode) 
        CHKERRQ(ierrcode)
        call VecGetOwnershipRange(b_flow, istart_flow, iend_flow,    &
                                  ierrcode)
        CHKERRQ(ierrcode)
        call VecGetOwnershipRanges(b_flow, ranges_flow, ierrcode)
        CHKERRQ(ierrcode)
        
        do i = 1, nprcs
            displs_nb_flow(i) = ranges_flow(i)
            scounts_nb_flow(i) = ranges_flow(i+1) - ranges_flow(i)    
        end do        
        rcounts_nb_flow = scounts_nb_flow(rank+1)
       
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Allocate space for RHS b finished."
                write(idbg,"(a)") "Flow: Allocate space for RHS b finished."
#ifdef MPI_SHARED
            end if
#endif
            write(*,"(a, 3(1x, i8))") "Flow: rank, range start, range end: ", rank, ranges_flow(rank+1)+1, ranges_flow(rank+2)
            call petsc_set_barrier
        end if
        
        !duplicate vector for the result x
        call VecDuplicate(b_flow, x_flow, ierrcode) 
        CHKERRQ(ierrcode)
        
        !duplicate vector for u
        call VecDuplicate(x_flow, u_flow, ierrcode) 
        CHKERRQ(ierrcode)
        
        !initialize indices
        allocate(ix_flow(iend_flow - istart_flow))
        do i = 1, iend_flow - istart_flow
            ix_flow(i)= istart_flow + i - 1
        end do
        
        !Allocate space for ia
        allocate(ia_flow(rcounts_nb_flow+1))        
       
#ifdef MPI        
        call MPI_SCATTERV(ia_in, scounts_nb_flow+1, displs_nb_flow,  &
                          MPI_INTEGER4, ia_flow, rcounts_nb_flow+1,  &
                          MPI_INTEGER4, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        ia_flow = ia_in    
#endif

        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Allocate space for ia finished."
                write(idbg,"(a)") "Flow: Allocate space for ia finished."
#ifdef MPI_SHARED
            end if 
#endif
            write(*,"(a, 3(1x, i8))") "Flow: rank, ia start, ia end: ", rank, ia_flow(1), ia_flow(size(ia_flow, 1))
            call petsc_set_barrier
        end if
#ifdef MPI_SHARED
        if(rank == 0) then 
#endif
            do i = 1, nprcs
                displs_nnz_flow(i) = ia_in(ranges_flow(i)+1)-1
                scounts_nnz_flow(i) = ia_in(ranges_flow(i+1)+1)-ia_in(ranges_flow(i)+1)           
            end do
#ifdef MPI_SHARED
        end if 
#endif
        
#ifdef MPI        
        call MPI_BCAST (displs_nnz_flow, nprcs, MPI_INTEGER4, 0,     &
                        Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
        call MPI_BCAST (scounts_nnz_flow, nprcs, MPI_INTEGER4, 0,    &
                        Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#endif
        
        rcounts_nnz_flow = scounts_nnz_flow(rank+1)
        
        !Allocate space for ja
        allocate(ja_flow(ia_flow(size(ia_flow,1))-ia_flow(1)))
#ifdef MPI     
        call MPI_SCATTERV(ja_in, scounts_nnz_flow, displs_nnz_flow,  &
                          MPI_INTEGER4, ja_flow, rcounts_nnz_flow,   &
                          MPI_INTEGER4, 0, Petsc_Comm_World ,        &
                          ierrcode)
        CHKERRQ(ierrcode)
#else
        ja_flow = ja_in
#endif
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Allocate space for ja finished."
                write(idbg,"(a)") "Flow: Allocate space for ja finished."
#ifdef MPI_SHARED
            end if
#endif
            write(*,"(a, 3(1x, i8))") "Flow: rank, ja start, ja end: ", rank, ja_flow(1), ja_flow(size(ja_flow, 1))
            call petsc_set_barrier
        end if
        
        !Allocate space for a
        allocate(d_nnz(rcounts_nb_flow))
        d_nnz = 0
        
        allocate(o_nnz(rcounts_nb_flow))
        o_nnz = 0
        
        
        k = 0
        do i = 1, iend_flow - istart_flow
            do j = 1, ia_flow(i+1)-ia_flow(i)
                k = k + 1
                if(ja_flow(k) > istart_flow .and.                    &
                   ja_flow(k) <= iend_flow) then
                    d_nnz(i) = d_nnz(i) + 1
                else
                    o_nnz(i) = o_nnz(i) + 1
                end if
            end do
        end do
        
        call MatCreateAIJ(Petsc_Comm_World, PETSC_DECIDE,            &
                          PETSC_DECIDE, nb_in, nb_in,                &
                          Petsc_Null_Integer,                        &
                          d_nnz(1:iend_flow-istart_flow),            &
                          Petsc_Null_Integer,                        &
                          o_nnz(1:iend_flow-istart_flow),            &
                          a_flow, ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Allocate space for a finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
       
        !Create linear solver context
        call KSPCreate(Petsc_Comm_World,ksp_flow,ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: KSPCreate finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set PC type
        call KSPGetPC(ksp_flow, pc_flow, ierrcode)
        CHKERRQ(ierrcode)
        
        if(trim(strPCType_flow) == "pcnone" .or. &
           trim(strPCType_flow) == "none") then
          call PCSetType(pc_flow,PCNONE, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_flow) == "pcjacobi" .or. &
                trim(strPCType_flow) == "jacobi") then
          call PCSetType(pc_flow,PCJACOBI, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_flow) == "pclu" .or. &
                trim(strPCType_flow) == "lu") then
          call PCSetType(pc_flow,PCLU, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_flow) == "pcbjacobi" .or. &
               trim(strPCType_flow) == "bjacobi") then
          call PCSetType(pc_flow,PCBJACOBI, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_flow) == "pcilu" .or. &
                trim(strPCType_flow) == "ilu") then
          call PCSetType(pc_flow,PCILU, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_flow) == "pcasm" .or. &
                trim(strPCType_flow) == "asm") then
          call PCSetType(pc_flow,PCASM, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_flow) == "pcksp" .or. &
                trim(strPCType_flow) == "ksp") then
          call PCSetType(pc_flow,PCKSP, ierrcode)
          CHKERRQ(ierrcode)
        else
          call PCSetType(pc_flow,PCBJACOBI, ierrcode)
          CHKERRQ(ierrcode)
        end if
        
        !Set KSP Solver type
        if (trim(strKSPType_flow) == "kspgmres" .or. &
            trim(strKSPType_flow) == "gmres") then
          call KSPSetType(ksp_flow, KSPGMRES, ierrcode)
          CHKERRQ(ierrcode)
        else if (trim(strKSPType_flow) == "kspbcgs" .or. &
                 trim(strKSPType_flow) == "bcgs") then
          call KSPSetType(ksp_flow, KSPBCGS, ierrcode)
          CHKERRQ(ierrcode)
        else
          call KSPSetType(ksp_flow, KSPGMRES, ierrcode)
          CHKERRQ(ierrcode)
        end if
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: KSPSetType finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set linear solver defaults for this problem (optional).
        
        
        call KSPSetTolerances(ksp_flow, rtol_flow, abstol_flow,      &
                              dtol_flow, maxits_flow, ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: KSPSetTolerances finished."
                write(*,"(a, 4(1x, e15.7))") "rtol, abstol, dtol, maxits", rtol_flow, abstol_flow, dtol_flow, maxits_flow
                write(idbg,"(a)") "Flow: KSPSetTolerances finished."
                write(idbg,"(a, 4(1x, e15.7))") "rtol, abstol, dtol, maxits", rtol_flow, abstol_flow, dtol_flow, maxits_flow     
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set solver options
        call KSPSetFromOptions(ksp_flow, ierrcode)  
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: KSPSetFromOptions finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set convergence test routine
        if (trim(strKSPConvergenceType_flow) == "kspuserdefined" .or. &
            trim(strKSPConvergenceType_flow) == "userdefined") then
            b_mykspconverged_flow = .true.
            call KSPSetConvergenceTest(ksp_flow,mykspconverged_flow,      &
                 PETSC_NULL_OBJECT,PETSC_NULL_FUNCTION,ierrcode)
            CHKERRQ(ierrcode)
        else    
            b_mykspconverged_flow = .false.
        end if
        
        !Set local space for a_flow_loc, b_flow_loc and x_flow_loc.
        !This part should be improved.
        allocate(a_flow_loc(scounts_nnz_flow(rank+1)))
        allocate(b_flow_loc(scounts_nb_flow(rank+1)))
        allocate(x_flow_loc(scounts_nb_flow(rank+1)))
        
        bSetMatOption_flow = .true.
        
        deallocate(d_nnz)
        deallocate(o_nnz)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: petsc_solver_create_flow finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        b_initialized_flow = .true.
        
    end subroutine petsc_solver_create_flow

    !>
    !> petsc_solver_ksp_flow(a_in, b_in, x_out)
    !> Solve flow equations by PETSc ksp solver
    !> @ilog_in log file unit
    !> @idetail_in solver information level
    !> @a_in matrix a values
    !> @b_in right-hand-side b values
    !> @x_out solution vector x
    !> @itsolv_out number of solver iterations
    !> @res_out residual
    !> @overflow_out overflow flag
    !> @rnorm_out residual 2-norm
    !>
    subroutine petsc_solver_ksp_flow(ilog_in, idetail_in, a_in, b_in, &
                                     x_out, itsolv_out, overflow_out, &
                                     rnorm_out)
    
        implicit none
        
        PetscInt, intent(in) :: ilog_in
        PetscInt, intent(in) :: idetail_in
        PetscReal, allocatable, intent(in) :: a_in(:) 
        PetscReal, allocatable, intent(in) :: b_in(:) 
        PetscReal, allocatable, intent(inout) :: x_out(:)
                
        PetscInt, intent(out)  :: itsolv_out
        PetscBool, intent(out) :: overflow_out
        PetscReal, intent(out) :: rnorm_out
        
        !PetscBool, intent(in) :: b_outx
        !character(*), intent(in) :: str_outx
        
        
        !local variable
        integer*4 :: i, i1, j, k
        
        info_debug = 0
        
        itsolv_out = 0
        overflow_out = .false.
        rnorm_out = 0.0
#ifdef MPI        
        call MPI_SCATTERV(a_in, scounts_nnz_flow, displs_nnz_flow,   &
                          MPI_REAL8, a_flow_loc, rcounts_nnz_flow,   &
                          MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        a_flow_loc = a_in
#endif        
        !Set matrix elements
        ! - Each processor needs to insert only elements that it owns
        !   locally (but any non-local elements will be sent to the
        !   appropriate processor during matrix assembly).
        ! - Always specify global row and columns of matrix entries.
        ! - Note that MatSetValues() uses 0-based row and column numbers
        !   in Fortran as well as in C.        
        call MatZeroEntries(a_flow, ierrcode) 
        CHKERRQ(ierrcode)
        
        i1 = ia_flow(1)
        do i = 1, iend_flow - istart_flow
            j = ia_flow(i)
            k = ia_flow(i+1) 
            call MatSetValues(a_flow, 1, i+istart_flow-1, k-j,       &
                 ja_flow(j-i1+1:k-i1)-1, a_flow_loc(j-i1+1:k-i1),    &
                 Insert_Values, ierrcode)
            CHKERRQ(ierrcode)
        end do
        
        call MatAssemblyBegin(a_flow, Mat_Final_Assembly, ierrcode)
        CHKERRQ(ierrcode)

        call MatAssemblyEnd(a_flow, Mat_Final_Assembly, ierrcode)
        CHKERRQ(ierrcode)
        
        if(bSetMatOption_flow) then
            call MatSetOption(a_flow,Mat_New_Nonzero_location_Err,   &
                              Petsc_True,ierrcode)
            CHKERRQ(ierrcode)
            bSetMatOption_flow = .false.
        end if
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Mat a_flow assembly finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set right hand side b
#ifdef MPI        
        call MPI_SCATTERV(b_in, scounts_nb_flow, displs_nb_flow,     &
                          MPI_REAL8, b_flow_loc, rcounts_nb_flow,    &
                          MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        b_flow_loc = b_in
#endif
        
        call VecSetValues(b_flow, iend_flow-istart_flow, ix_flow,    &
                          b_flow_loc, Insert_Values, ierrcode) 
        CHKERRQ(ierrcode)
        
        call VecAssemblyBegin(b_flow, ierrcode)
        CHKERRQ(ierrcode)
        call VecAssemblyEnd(b_flow, ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Right hand side b assembly finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Solve
        call KSPSetOperators(ksp_flow, a_flow, a_flow,               &
                             SAME_NONZERO_PATTERN, ierrcode)
        CHKERRQ(ierrcode)
        call KSPSolve(ksp_flow, b_flow, x_flow, ierrcode)
        CHKERRQ(ierrcode)
        
        !Get residual norm, by default, preconditioned residual norm is calculated.
        if (b_mykspconverged_flow) then
            rnorm_out = rnorm_flow
        else
            call KSPGetResidualNorm(ksp_flow, rnorm_out, ierrcode)
            CHKERRQ(ierrcode)
        end if
        
        !!This operation is expensive, use -ksp_norm_type unpreconditioned instead
        !call MatMult(a_flow, x_flow, u_flow, ierrcode)
        !CHKERRQ(ierrcode)
        !call VecAXPY(u_flow, neg_one, b_flow, ierrcode)
        !CHKERRQ(ierrcode)
        !call VecNormBegin(u_flow, Norm_2, rnorm_out, ierrcode)
        !CHKERRQ(ierrcode)
        !call VecNormEnd(u_flow, Norm_2, rnorm_out, ierrcode)
        !CHKERRQ(ierrcode)
        
        
        !Get iteration and convergence information
        call KSPGetIterationNumber(ksp_flow, itsolv_out, ierrcode)
        CHKERRQ(ierrcode)
        if(itsolv_out == 0) then
            overflow_out = .true.
            return
        end if
        
        call KSPGetErrorIfNotConverged(ksp_flow, overflow_out,       &
                                       ierrcode)
        CHKERRQ(ierrcode)
        
        !Output the solution solved by PETSc Viewer
        !if(b_outx) then
        !    call PetscViewerASCIIOpen(Petsc_Comm_World, trim(str_outx) , viewer, ierr)
        !    CHKERRQ(ierr)
        !    call VecView(x_flow, viewer, ierr)
        !    CHKERRQ(ierr)
        !    call PetscViewerDestroy(viewer, ierr) 
        !    CHKERRQ(ierr)
        !end if
        
        if(idetail_in > 1) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                if(.not. overflow_out) then
                    write(ilog_in, 9)
                    write(ilog_in, 10)
                    write(ilog_in, 11)
                    write(ilog_in, 12) itsolv_out, rnorm_out
                end if
#ifdef MPI_SHARED
            end if
#endif
        end if
        
        if(overflow_out) then
            return
        end if
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: KSP Solver finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        if(info_debug > 0) then
            call PetscViewerASCIIOpen(Petsc_Comm_World, "xout_flow.txt" , &
                                      viewer, ierrcode)
            CHKERRQ(ierrcode)
            call VecView(x_flow, viewer, ierrcode)
            CHKERRQ(ierrcode)
            call PetscViewerDestroy(viewer, ierrcode)
            CHKERRQ(ierrcode)
        end if
        
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Export result to xout.txt finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
                
        
        !Return result
        call VecGetArrayF90(x_flow, vecpointer, ierrcode)
        CHKERRQ(ierrcode)
        if(info_debug > 0) then
            write(*,'(a, 3(1x, i))') "Flow: rank, sendcnt, recvcnt, displs: ", rank, rcounts_nb_flow, scounts_nb_flow(rank+1), displs_nb_flow(rank+1)
            write(*,'(a, 1x, i, 2(1x, f))') "Flow: rank, vecpointer size, start, end: ", rank, size(vecpointer, 1), vecpointer(1), vecpointer(size(vecpointer, 1))
            write(*,'(a, 1x, i)') "Flow: size of x_out: ", size(x_out,1)
            call petsc_set_barrier
        end if
#ifdef MPI        
        call MPI_Gatherv(vecpointer, rcounts_nb_flow, MPI_REAL8,     &
                         x_out, scounts_nb_flow, displs_nb_flow,     &
                         MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        x_out = vecpointer
#endif

        call VecRestoreArrayF90(x_flow,vecpointer,ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "Flow: Return result x finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
9     FORMAT (/1x,'Solver Iteration Convergence Summary:')
10    FORMAT (1x,'solver       residual')
11    FORMAT (1x,'iteration    norm')
12    FORMAT (1x,i5,4x,2x,1pd11.4)
    
    end subroutine petsc_solver_ksp_flow

    
    !>
    !> petsc_solver_create_react(nb, ia, ja)
    !> Create PETSc solver space for react problem
    !> @nb_in size of right hand side 
    !> @ia_in structure of sparse matrix ia
    !> @ja_in structure of sparse matrix ja
    !>
    !> Sample calling
    !>   if(reactive_transport) then
    !>       call petsc_solver_create_react(n*nn, iart, jart)
    !>   end if
    subroutine petsc_solver_create_react(nb_in, ia_in, ja_in)
    
        implicit none
        
        PetscInt, intent(in) :: nb_in
        PetscInt, allocatable, intent(in) :: ia_in(:)
        PetscInt, allocatable, intent(in) :: ja_in(:)
        
        !local variable
        integer*4 :: i, j, k
        PetscInt, allocatable :: d_nnz(:)
        PetscInt, allocatable :: o_nnz(:)
        
        info_debug = 1        
        
        allocate(ranges_react(nprcs+1))        
        allocate(displs_nb_react(nprcs))        
        allocate(scounts_nb_react(nprcs))        
        

        allocate(displs_nnz_react(nprcs))
        allocate(scounts_nnz_react(nprcs))
        
        !Broadcast nb_in
#ifdef MPI
        call MPI_BCAST (nb_in, 1, MPI_INTEGER4, 0, Petsc_Comm_World, &
                        ierrcode)
        CHKERRQ(ierrcode)
#endif

        !Allocate space for RHS b
        call VecCreateMPI(Petsc_Comm_World, Petsc_Decide, nb_in,     &
                          b_react, ierrcode) 
        CHKERRQ(ierrcode)

        call VecGetOwnershipRange(b_react, istart_react, iend_react,    &
                                  ierrcode)
        CHKERRQ(ierrcode)

        call VecGetOwnershipRanges(b_react, ranges_react, ierrcode)
        CHKERRQ(ierrcode)
        
        do i = 1, nprcs
            displs_nb_react(i) = ranges_react(i)
            scounts_nb_react(i) = ranges_react(i+1) - ranges_react(i)    
        end do        
        rcounts_nb_react = scounts_nb_react(rank+1)
       
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Allocate space for RHS b finished."
                write(idbg,"(a)") "React: Allocate space for RHS b finished."
#ifdef MPI_SHARED
            end if
#endif
            write(*,"(a, 3(1x, i8))") "React: rank, range start, range end: ", rank, ranges_react(rank+1)+1, ranges_react(rank+2)
            call petsc_set_barrier
        end if
        
        !duplicate vector for the result x
        call VecDuplicate(b_react, x_react, ierrcode) 
        CHKERRQ(ierrcode)
        
        !duplicate vector for u
        call VecDuplicate(x_react, u_react, ierrcode)
        CHKERRQ(ierrcode)
        
        !initialize indices
        allocate(ix_react(iend_react - istart_react))
        do i = 1, iend_react - istart_react
            ix_react(i)= istart_react + i - 1
        end do
        
        !Allocate space for ia
        allocate(ia_react(rcounts_nb_react+1))
#ifdef MPI
        call MPI_SCATTERV(ia_in, scounts_nb_react+1, displs_nb_react,  &
                          MPI_INTEGER4, ia_react, rcounts_nb_react+1,  &
                          MPI_INTEGER4, 0, Petsc_Comm_World, ierrcode) 
        CHKERRQ(ierrcode)
#else        
        ia_react = ia_in
#endif
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Allocate space for ia finished."
                write(idbg,"(a)") "React: Allocate space for ia finished."
                
                write(*,"(a, 3(1x, i8))") "size(ia_in), ia_in start, end ",      &
                          size(ia_in), ia_in(1), ia_in(size(ia_in,1))
#ifdef MPI_SHARED
            end if
#endif
            write(*,"(a, 4(1x, i8))") "React: rank, rcount, ia start, ia end: ", &
                 rank,rcounts_nb_react+1, ia_react(1), ia_react(size(ia_react, 1))

            call petsc_set_barrier
        end if
       
        !Allocate space for ja
        allocate(ja_react(ia_react(size(ia_react,1))-ia_react(1)))
#ifdef MPI_SHARED
        if(rank == 0) then 
#endif
            do i = 1, nprcs
                displs_nnz_react(i) = ia_in(ranges_react(i)+1)-1
                scounts_nnz_react(i) = ia_in(ranges_react(i+1)+1)-ia_in(ranges_react(i)+1)           
            end do 
#ifdef MPI_SHARED
        end if 
#endif
        
#ifdef MPI        
        call MPI_BCAST (displs_nnz_react, nprcs, MPI_INTEGER4, 0,     &
                        Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)

        call MPI_BCAST (scounts_nnz_react, nprcs, MPI_INTEGER4, 0,    &
                        Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#endif

        rcounts_nnz_react = scounts_nnz_react(rank+1)
        
#ifdef MPI        
        call MPI_SCATTERV(ja_in, scounts_nnz_react, displs_nnz_react,  &
                          MPI_INTEGER4, ja_react, rcounts_nnz_react,   &
                          MPI_INTEGER4, 0, Petsc_Comm_World ,          &
                          ierrcode) 
        CHKERRQ(ierrcode)
#else
        ja_react = ja_in
#endif
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Allocate space for ja finished."
                write(idbg,"(a)") "React: Allocate space for ja finished."
#ifdef MPI_SHARED
            end if
#endif
            write(*,"(a, 3(1x, i8))") "React: rank, ja start, ja end: ", rank, ja_react(1), ja_react(size(ja_react, 1))
            call petsc_set_barrier
        end if
        
        !Allocate space for a
        allocate(d_nnz(rcounts_nb_react))
        d_nnz = 0
        
        allocate(o_nnz(rcounts_nb_react))
        o_nnz = 0
        
        k = 0
        do i = 1, iend_react - istart_react
            do j = 1, ia_react(i+1)-ia_react(i)
                k = k + 1
                if(ja_react(k) > istart_react .and.                    &
                   ja_react(k) <= iend_react) then
                    d_nnz(i) = d_nnz(i) + 1
                else
                    o_nnz(i) = o_nnz(i) + 1
                end if
            end do
        end do
        
        call MatCreateAIJ(Petsc_Comm_World, PETSC_DECIDE,            &
                          PETSC_DECIDE, nb_in, nb_in,                &
                          Petsc_Null_Integer,                        &
                          d_nnz(1:iend_react-istart_react),          &
                          Petsc_Null_Integer,                        &
                          o_nnz(1:iend_react-istart_react),          &
                          a_react, ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Allocate space for a finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
       
        !Create linear solver context
        call KSPCreate(Petsc_Comm_World,ksp_react,ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: KSPCreate finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set PC type
        call KSPGetPC(ksp_react, pc_react, ierrcode)
        CHKERRQ(ierrcode)
        
        if(trim(strPCType_react) == "pcnone" .or. &
           trim(strPCType_react) == "none") then
          call PCSetType(pc_react,PCNONE, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_react) == "pcjacobi" .or. &
                trim(strPCType_react) == "jacobi") then
          call PCSetType(pc_react,PCJACOBI, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_react) == "pclu" .or. &
                trim(strPCType_react) == "lu") then
          call PCSetType(pc_react,PCLU, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_react) == "pcbjacobi" .or. &
                trim(strPCType_react) == "bjacobi") then
          call PCSetType(pc_react,PCBJACOBI, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_react) == "pcilu" .or. &
                trim(strPCType_react) == "ilu") then
          call PCSetType(pc_react,PCILU, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_react) == "pcasm" .or. &
                trim(strPCType_react) == "asm") then
          call PCSetType(pc_react,PCASM, ierrcode)
          CHKERRQ(ierrcode)
        else if(trim(strPCType_react) == "pcksp" .or. &
                trim(strPCType_react) == "ksp") then
          call PCSetType(pc_react,PCKSP, ierrcode)
          CHKERRQ(ierrcode)
        else
          call PCSetType(pc_react,PCBJACOBI, ierrcode)
          CHKERRQ(ierrcode)
        end if
        
        !Set KSP Solver type
        if (trim(strKSPType_react) == "kspgmres" .or. &
            trim(strKSPType_react) == "gmres") then
          call KSPSetType(ksp_react, KSPGMRES, ierrcode)
          CHKERRQ(ierrcode)
        else if (trim(strKSPType_react) == "kspbcgs" .or. &
                 trim(strKSPType_react) == "bcgs") then
          call KSPSetType(ksp_react, KSPBCGS, ierrcode)
          CHKERRQ(ierrcode)
        else
          call KSPSetType(ksp_react, KSPGMRES, ierrcode)
          CHKERRQ(ierrcode)
        end if
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: KSPSetType finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
       
        !Set linear solver defaults for this problem (optional).
        
        
        call KSPSetTolerances(ksp_react, rtol_react, abstol_react,      &
                              dtol_react, maxits_react, ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: KSPSetTolerances finished."
                write(*,"(a, 4(1x, e15.7))") "rtol, abstol, dtol, maxits", rtol_react, abstol_react, dtol_react, maxits_react
                write(idbg,"(a)") "React: KSPSetTolerances finished."
                write(idbg,"(a, 4(1x, e15.7))") "rtol, abstol, dtol, maxits", rtol_react, abstol_react, dtol_react, maxits_react
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set solver options
        call KSPSetFromOptions(ksp_react, ierrcode)  
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: KSPSetFromOptions finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set convergence test routine
        if (trim(strKSPConvergenceType_react) == "kspuserdefined" .or. &
            trim(strKSPConvergenceType_react) == "userdefined") then
            b_mykspconverged_react = .true.
            call KSPSetConvergenceTest(ksp_react,mykspconverged_react,      &
                 PETSC_NULL_OBJECT,PETSC_NULL_FUNCTION,ierrcode)
            CHKERRQ(ierrcode)
        else    
            b_mykspconverged_react = .false.
        end if
        
        !Set local space for a_react_loc, b_react_loc and x_react_loc.
        !This part should be improved.
        allocate(a_react_loc(scounts_nnz_react(rank+1)))
        allocate(b_react_loc(scounts_nb_react(rank+1)))
        allocate(x_react_loc(scounts_nb_react(rank+1)))
        
        bSetMatOption_react = .true.
        
        deallocate(d_nnz)
        deallocate(o_nnz)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: petsc_solver_create_react finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        b_initialized_react = .true.
        
    end subroutine petsc_solver_create_react

    !>
    !> petsc_solver_ksp_react(a_in, b_in, x_out)
    !> Solve react equations by PETSc ksp solver
    !> @ilog_in log file unit
    !> @idetail_in solver information level
    !> @a_in matrix a values
    !> @b_in right-hand-side b values
    !> @x_out solution vector x
    !> @itsolv_out number of solver iterations
    !> @res_out residual
    !> @overflow_out overreact flag
    !> @rnorm_out residual 2-norm
    !>
    subroutine petsc_solver_ksp_react(ilog_in, idetail_in, a_in, b_in, &
                                     x_out, itsolv_out, overflow_out, &
                                     rnorm_out)
    
        implicit none
        
        PetscInt, intent(in) :: ilog_in
        PetscInt, intent(in) :: idetail_in
        PetscReal, allocatable, intent(in) :: a_in(:) 
        PetscReal, allocatable, intent(in) :: b_in(:) 
        PetscReal, allocatable, intent(inout) :: x_out(:)
        
        PetscInt, intent(out)  :: itsolv_out
        PetscBool, intent(out) :: overflow_out
        PetscReal, intent(out) :: rnorm_out
        
        !PetscBool, intent(in) :: b_outx
        !character(*), intent(in) :: str_outx
        
        !local variable
        integer*4 :: i, i1, j, k
        
        info_debug = 0
        
        itsolv_out = 0
        overflow_out = .false.
        rnorm_out = 0.0  
        
#ifdef MPI       
        call MPI_SCATTERV(a_in, scounts_nnz_react, displs_nnz_react, &
                          MPI_REAL8, a_react_loc, rcounts_nnz_react, &
                          MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        a_react_loc = a_in
#endif
        
        !Set matrix elements
        ! - Each processor needs to insert only elements that it owns
        !   locally (but any non-local elements will be sent to the
        !   appropriate processor during matrix assembly).
        ! - Always specify global row and columns of matrix entries.
        ! - Note that MatSetValues() uses 0-based row and column numbers
        !   in Fortran as well as in C.        
        call MatZeroEntries(a_react, ierrcode) 
        CHKERRQ(ierrcode)
        
        i1 = ia_react(1)
        do i = 1, iend_react - istart_react
            j = ia_react(i)
            k = ia_react(i+1) 
            call MatSetValues(a_react, 1, i+istart_react-1, k-j,       &
                 ja_react(j-i1+1:k-i1)-1, a_react_loc(j-i1+1:k-i1),    &
                 Insert_Values, ierrcode)
            CHKERRQ(ierrcode)
        end do
        
        call MatAssemblyBegin(a_react, Mat_Final_Assembly, ierrcode)
        CHKERRQ(ierrcode)
        call MatAssemblyEnd(a_react, Mat_Final_Assembly, ierrcode)
        CHKERRQ(ierrcode)
        
        if(bSetMatOption_react) then
            call MatSetOption(a_react,Mat_New_Nonzero_location_Err,   &
                              Petsc_True,ierrcode)
            CHKERRQ(ierrcode)
            bSetMatOption_react = .false.
        end if
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Mat a_react assembly finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Set right hand side b
#ifdef MPI
        call MPI_SCATTERV(b_in, scounts_nb_react, displs_nb_react,     &
                          MPI_REAL8, b_react_loc, rcounts_nb_react,    &
                          MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        b_react_loc = b_in
#endif
        
        call VecSetValues(b_react, iend_react-istart_react, ix_react,    &
                          b_react_loc, Insert_Values, ierrcode) 
        CHKERRQ(ierrcode)
        
        call VecAssemblyBegin(b_react, ierrcode)
        CHKERRQ(ierrcode)
        call VecAssemblyEnd(b_react, ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Right hand side b assembly finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
        !Solve
        call KSPSetOperators(ksp_react, a_react, a_react,            &
                             SAME_NONZERO_PATTERN, ierrcode)
        CHKERRQ(ierrcode)
        call KSPSolve(ksp_react, b_react, x_react, ierrcode)
        CHKERRQ(ierrcode)
        
        !Get residual norm, by default, preconditioned residual norm is calculated.
        if(b_mykspconverged_react) then
            rnorm_out = rnorm_react
        else        
            call KSPGetResidualNorm(ksp_react, rnorm_out, ierrcode)
            CHKERRQ(ierrcode)
        end if
        
        !!Get true residual norm
        !!This operation is expensive, use -ksp_norm_type unpreconditioned instead
        !call MatMult(a_react, x_react, u_react, ierrcode)
        !CHKERRQ(ierrcode)
        !call VecAXPY(u_react, neg_one, b_react, ierrcode)
        !CHKERRQ(ierrcode)
        !call VecNormBegin(u_react, Norm_2, rnorm_out, ierrcode)
        !CHKERRQ(ierrcode)
        !call VecNormEnd(u_react, Norm_2, rnorm_out, ierrcode)
        !CHKERRQ(ierrcode)
        
        !Get iteration and convergence information
        call KSPGetIterationNumber(ksp_react, itsolv_out, ierrcode)
        CHKERRQ(ierrcode)
        if(itsolv_out == 0) then
            overflow_out = .true.            
            return
        end if
        call KSPGetErrorIfNotConverged(ksp_react, overflow_out,      &
                                       ierrcode)
        CHKERRQ(ierrcode)
        
        !Output the solution solved by PETSc Viewer
        !if(b_outx) then
        !    call PetscViewerASCIIOpen(Petsc_Comm_World, trim(str_outx) , viewer, ierrcode)
        !    CHKERRQ(ierrcode)
        !    call VecView(x_react, viewer, ierrcode)
        !    CHKERRQ(ierrcode)
        !    call PetscViewerDestroy(viewer, ierrcode) 
        !    CHKERRQ(ierrcode)
        !end if
        
        if(idetail_in > 1) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                if(.not. overflow_out) then
                    write(ilog_in, 9)
                    write(ilog_in, 10)
                    write(ilog_in, 11)
                    write(ilog_in, 12) itsolv_out, rnorm_out
                end if
#ifdef MPI_SHARED
            end if
#endif
        end if
        
        if(overflow_out) then
            return
        end if
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: KSP Solver finished."
#ifdef MPI_SHARED    
            end if
#endif
            call petsc_set_barrier
        end if
        
        if(info_debug > 0) then
            call PetscViewerASCIIOpen(Petsc_Comm_World, "xout_react.txt" , &
                                      viewer, ierrcode)
            CHKERRQ(ierrcode)
            call VecView(x_react, viewer, ierrcode)
            CHKERRQ(ierrcode)
            call PetscViewerDestroy(viewer, ierrcode)
            CHKERRQ(ierrcode)
        end if
        
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Export result to xout.txt finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
                
        
        !Return result
        call VecGetArrayF90(x_react, vecpointer, ierrcode)
        CHKERRQ(ierrcode)
        if(info_debug > 0) then
            write(*,'(a, 3(1x, i))') "React: rank, sendcnt, recvcnt, displs: ", rank, rcounts_nb_react, scounts_nb_react(rank+1), displs_nb_react(rank+1)
            write(*,'(a, 1x, i, 2(1x, f))') "React: rank, vecpointer size, start, end: ", rank, size(vecpointer, 1), vecpointer(1), vecpointer(size(vecpointer, 1))
            write(*,'(a, 1x, i)') "React: size of x_out: ", size(x_out,1)
            call petsc_set_barrier
        end if
        
#ifdef MPI
        call MPI_Gatherv(vecpointer, rcounts_nb_react, MPI_REAL8,     &
                         x_out, scounts_nb_react, displs_nb_react,     &
                         MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)
#else
        x_out = vecpointer
#endif
        call VecRestoreArrayF90(x_react,vecpointer,ierrcode)
        CHKERRQ(ierrcode)
        
        if(info_debug > 0) then
#ifdef MPI_SHARED
            if(rank == 0) then
#endif
                write(*,"(a)") "React: Return result x finished."
#ifdef MPI_SHARED
            end if
#endif
            call petsc_set_barrier
        end if
        
9     FORMAT (/1x,'Solver Iteration Convergence Summary:')
10    FORMAT (1x,'solver       residual')
11    FORMAT (1x,'iteration    norm')
12    FORMAT (1x,i5,4x,2x,1pd11.4)
    
    end subroutine petsc_solver_ksp_react
   
    
    !>
    !> petsc_solver_release
    !> Release petsc solver memory
    !>
    subroutine petsc_solver_release
    
        implicit none
        
        if(b_initialized_flow) then
            
            deallocate(ranges_flow)        
            deallocate(displs_nb_flow)        
            deallocate(scounts_nb_flow)
            deallocate(displs_nnz_flow)
            deallocate(scounts_nnz_flow)
            deallocate(ia_flow)        
            deallocate(ja_flow)        
            deallocate(ix_flow)        
            deallocate(a_flow_loc)
            deallocate(b_flow_loc)
            deallocate(x_flow_loc)
        
            call MatDestroy(a_flow, ierrcode)
            CHKERRQ(ierrcode)
            call VecDestroy(b_flow, ierrcode)
            CHKERRQ(ierrcode)
            call VecDestroy(x_flow, ierrcode)
            CHKERRQ(ierrcode)
            call VecDestroy(u_flow, ierrcode)
            CHKERRQ(ierrcode)
            call KSPDestroy(ksp_flow, ierrcode)
            CHKERRQ(ierrcode)
            
        end if
        
        if(b_initialized_react) then
            
            deallocate(ranges_react)        
            deallocate(displs_nb_react)        
            deallocate(scounts_nb_react)
            deallocate(displs_nnz_react)
            deallocate(scounts_nnz_react)
            deallocate(ia_react)        
            deallocate(ja_react)        
            deallocate(ix_react)        
            deallocate(a_react_loc)
            deallocate(b_react_loc)
            deallocate(x_react_loc)
        
            call MatDestroy(a_react, ierrcode)
            CHKERRQ(ierrcode)
            call VecDestroy(b_react, ierrcode)
            CHKERRQ(ierrcode)
            call VecDestroy(x_react, ierrcode)
            CHKERRQ(ierrcode)
            call VecDestroy(u_react, ierrcode)
            CHKERRQ(ierrcode)
            call KSPDestroy(ksp_react, ierrcode)
            CHKERRQ(ierrcode)
        
        end if
    
    end subroutine petsc_solver_release
    
    !>
    !> petsc_set_finalize
    !>
    subroutine petsc_set_finalize
    
        implicit none
        
        call PetscFinalize(ierrcode)  
        CHKERRQ(ierrcode)
    
    end subroutine petsc_set_finalize
    
end module solver_petsc
    
#endif    
